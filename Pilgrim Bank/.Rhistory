j=j+1
}
cv.error
set.seed(20)
#divido in 10 gruppi che ovviamente non saranno della stessa cardinalità
cv.error.10=rep(0,8)
j=1
for (i in potenze){
glm.fit=glm(rating~poly(prevratings,i),data=CBC)
cv.error.10[j]=cv.glm(CBC,glm.fit,K=10)$delta[1]
j=j+1
}
cv.error.10
detach(CBC)
library(leaps)
CBC$network=as.factor(CBC$network)
CBC$day=as.factor(CBC$day)
CBC$month=as.factor(CBC$month)
CBCr=CBC[, 1:7]
regfit.full=regsubsets(rating~.,CBCr,nvmax=15)
summary(regfit.full)
reg.summary=summary(regfit.full)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
t=which.max(reg.summary$adjr2)
points(t,reg.summary$adjr2[t], col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
q=which.min(reg.summary$cp)
points(q,reg.summary$cp[q],col="red",cex=2,pch=20)
u=which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(u,reg.summary$bic[u],col="red",cex=2,pch=20)
set.seed(1)
#selezioniamo a caso tra le osservazioni a nostra disposizione
train=sample(c(TRUE,FALSE), nrow(CBCr),rep=TRUE)
test=(!train) #complemento
regfit.best=regsubsets(rating~.,data=CBCr[train,],nvmax=15)
test.mat=model.matrix(rating~.,data=CBCr[test,])
#estrae dal dateset è la matrice X, potrebbe servire per i calcoli
val.errors=rep(NA,15)
for(i in 1:15){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
#prodotto righe per colonne
val.errors[i]=mean((CBCr$rating[test]-pred)^2)
}
val.errors
min=which.min(val.errors)
min
coef(regfit.best,min)
#attributes(CBCr)
regfit.fwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="backward")
summary(regfit.bwd)
coef(regfit.full,6)
coef(regfit.fwd,6)
coef(regfit.bwd,6)
k=10
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
set.seed(1)
folds=sample(1:k,nrow(CBCr),replace=TRUE)
cv.errors=matrix(NA,k,15, dimnames=list(NULL, paste(1:15)))
for(j in 1:k){
best.fit=regsubsets(rating~.,data=CBCr[folds!=j,],nvmax=15)
for(i in 1:15){
pred=predict(best.fit,CBCr[folds==j,],id=i)
cv.errors[j,i]=mean( (CBC$rating[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
min=which.min(mean.cv.errors)
min
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
reg.best=regsubsets(rating~.,data=CBCr, nvmax=15)
coef(reg.best,min)
library(glmnet)
CBC = read.csv(file = "Dati per caso Colonial Broadcasting.csv", header=T)
x=model.matrix(rating~.,CBCr)[,-1] #tolgo l'intercetta perchè Beta_0 non va penalizzato
y=CBC$rating
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out, label = TRUE)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.coef
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.coef
knitr::opts_chunk$set(echo = TRUE)
rm(list =ls())
set.seed(1)
CBC = read.csv(file = "Dati per caso Colonial Broadcasting.csv", header=T)
attach(CBC)
dim(CBC)
names(CBC)
head(CBC)
sum(is.na(rating))
hist(rating)
mean(rating)
train=sample(88,88*0.7)
lm.fit=lm(rating~prevratings,data=CBC,subset=train)
summary(lm.fit)
plot(prevratings, rating)
abline(lm.fit, col="red")
mean((rating-predict(lm.fit,CBC))[-train]^2)
potenze = c(1,2,3,4,5,6,10,16)
ma<-matrix(nrow=3,ncol=length(potenze)+1)
ma[1,1]="potenze"
ma[2,1]="MSE"
ma[3,1]="R^2"
ma[1,2:(length(potenze)+1)]=potenze
for (i in 2:(length(potenze)+1)){
pwr = potenze[i-1]
lm.fit_n= lm(rating~poly(prevratings,pwr),data=CBC,subset=train)
ma[2,i] = mean((rating-predict(lm.fit_n,CBC))[-train]^2)
ma[3,i] = summary(lm.fit_n)$r.squared
}
ma
sample.size = length(prevratings) #decido la dimensione del campione
cv.errors = numeric(sample.size) #prealloco un vettore di zeri
for (k in 1:sample.size){
fitCV=lm(rating~prevratings,data=CBC[-k,]) #tolgo un'osservazione
cv.errors[k]= ((rating-predict(fitCV,CBC))[k])^2
#vado a prevedere solo per quell'osservazione
}
mean(cv.errors) #calcolo la media
library(boot)
glm.fit=glm(rating~prevratings,data=CBC)
cv.err=cv.glm(CBC,glm.fit) #passa la struttura dati e il modello
cv.err$K
cv.err$delta
cv.error=rep(0,8)
j=1
for (i in potenze){ #LOOCV su 8 modelli
glm.fit=glm(rating~poly(prevratings,i),data=CBC)
cv.error[j]=cv.glm(CBC,glm.fit)$delta[1]
j=j+1
}
cv.error
set.seed(20)
#divido in 10 gruppi che ovviamente non saranno della stessa cardinalità
cv.error.10=rep(0,8)
j=1
for (i in potenze){
glm.fit=glm(rating~poly(prevratings,i),data=CBC)
cv.error.10[j]=cv.glm(CBC,glm.fit,K=10)$delta[1]
j=j+1
}
cv.error.10
detach(CBC)
library(leaps)
CBC$network=as.factor(CBC$network)
CBC$day=as.factor(CBC$day)
CBC$month=as.factor(CBC$month)
CBCr=CBC[, 1:7]
regfit.full=regsubsets(rating~.,CBCr,nvmax=15)
summary(regfit.full)
reg.summary=summary(regfit.full)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
t=which.max(reg.summary$adjr2)
points(t,reg.summary$adjr2[t], col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
q=which.min(reg.summary$cp)
points(q,reg.summary$cp[q],col="red",cex=2,pch=20)
u=which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(u,reg.summary$bic[u],col="red",cex=2,pch=20)
set.seed(1)
#selezioniamo a caso tra le osservazioni a nostra disposizione
train=sample(c(TRUE,FALSE), nrow(CBCr),rep=TRUE)
test=(!train) #complemento
regfit.best=regsubsets(rating~.,data=CBCr[train,],nvmax=15)
test.mat=model.matrix(rating~.,data=CBCr[test,])
#estrae dal dateset è la matrice X, potrebbe servire per i calcoli
val.errors=rep(NA,15)
for(i in 1:15){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
#prodotto righe per colonne
val.errors[i]=mean((CBCr$rating[test]-pred)^2)
}
val.errors
min=which.min(val.errors)
min
coef(regfit.best,min)
#attributes(CBCr)
regfit.fwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="backward")
summary(regfit.bwd)
coef(regfit.full,6)
coef(regfit.fwd,6)
coef(regfit.bwd,6)
k=10
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
set.seed(1)
folds=sample(1:k,nrow(CBCr),replace=TRUE)
cv.errors=matrix(NA,k,15, dimnames=list(NULL, paste(1:15)))
for(j in 1:k){
best.fit=regsubsets(rating~.,data=CBCr[folds!=j,],nvmax=15)
for(i in 1:15){
pred=predict(best.fit,CBCr[folds==j,],id=i)
cv.errors[j,i]=mean( (CBC$rating[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
min=which.min(mean.cv.errors)
min
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
reg.best=regsubsets(rating~.,data=CBCr, nvmax=15)
coef(reg.best,min)
library(glmnet)
CBC = read.csv(file = "Dati per caso Colonial Broadcasting.csv", header=T)
x=model.matrix(rating~.,CBCr)[,-1] #tolgo l'intercetta perchè Beta_0 non va penalizzato
y=CBC$rating
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.coef
plot(lasso.mod, xvar="lambda")
plot(lasso.mod, xvar="dev")
plot(lasso.mod, xvar="lambda")
plot(lasso.mod, xvar="dev")
plot(lasso.mod)
plot(lasso.mod, xvar="lambda")
plot(ridge.mod)
In conclusione possiamo dire che il modello migliore è il modello ridge con $\lambda = 2.158988$ che ha MSE = 3.660313.
knitr::opts_chunk$set(echo = TRUE)
rm(list =ls())
set.seed(1)
CBC = read.csv(file = "Dati per caso Colonial Broadcasting.csv", header=T)
attach(CBC)
dim(CBC)
names(CBC)
head(CBC)
sum(is.na(rating))
hist(rating)
mean(rating)
train=sample(88,88*0.7)
lm.fit=lm(rating~prevratings,data=CBC,subset=train)
summary(lm.fit)
plot(prevratings, rating)
abline(lm.fit, col="red")
mean((rating-predict(lm.fit,CBC))[-train]^2)
potenze = c(1,2,3,4,5,6,10,16)
ma<-matrix(nrow=3,ncol=length(potenze)+1)
ma[1,1]="potenze"
ma[2,1]="MSE"
ma[3,1]="R^2"
ma[1,2:(length(potenze)+1)]=potenze
for (i in 2:(length(potenze)+1)){
pwr = potenze[i-1]
lm.fit_n= lm(rating~poly(prevratings,pwr),data=CBC,subset=train)
ma[2,i] = mean((rating-predict(lm.fit_n,CBC))[-train]^2)
ma[3,i] = summary(lm.fit_n)$r.squared
}
ma
sample.size = length(prevratings) #decido la dimensione del campione
cv.errors = numeric(sample.size) #prealloco un vettore di zeri
for (k in 1:sample.size){
fitCV=lm(rating~prevratings,data=CBC[-k,]) #tolgo un'osservazione
cv.errors[k]= ((rating-predict(fitCV,CBC))[k])^2
#vado a prevedere solo per quell'osservazione
}
mean(cv.errors) #calcolo la media
library(boot)
glm.fit=glm(rating~prevratings,data=CBC)
cv.err=cv.glm(CBC,glm.fit) #passa la struttura dati e il modello
cv.err$K
cv.err$delta
cv.error=rep(0,8)
j=1
for (i in potenze){ #LOOCV su 8 modelli
glm.fit=glm(rating~poly(prevratings,i),data=CBC)
cv.error[j]=cv.glm(CBC,glm.fit)$delta[1]
j=j+1
}
cv.error
set.seed(20)
#divido in 10 gruppi che ovviamente non saranno della stessa cardinalità
cv.error.10=rep(0,8)
j=1
for (i in potenze){
glm.fit=glm(rating~poly(prevratings,i),data=CBC)
cv.error.10[j]=cv.glm(CBC,glm.fit,K=10)$delta[1]
j=j+1
}
cv.error.10
detach(CBC)
library(leaps)
CBC$network=as.factor(CBC$network)
CBC$day=as.factor(CBC$day)
CBC$month=as.factor(CBC$month)
CBCr=CBC[, 1:7]
regfit.full=regsubsets(rating~.,CBCr,nvmax=15)
summary(regfit.full)
reg.summary=summary(regfit.full)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
t=which.max(reg.summary$adjr2)
points(t,reg.summary$adjr2[t], col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
q=which.min(reg.summary$cp)
points(q,reg.summary$cp[q],col="red",cex=2,pch=20)
u=which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(u,reg.summary$bic[u],col="red",cex=2,pch=20)
set.seed(1)
#selezioniamo a caso tra le osservazioni a nostra disposizione
train=sample(c(TRUE,FALSE), nrow(CBCr),rep=TRUE)
test=(!train) #complemento
regfit.best=regsubsets(rating~.,data=CBCr[train,],nvmax=15)
test.mat=model.matrix(rating~.,data=CBCr[test,])
#estrae dal dateset è la matrice X, potrebbe servire per i calcoli
val.errors=rep(NA,15)
for(i in 1:15){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
#prodotto righe per colonne
val.errors[i]=mean((CBCr$rating[test]-pred)^2)
}
val.errors
min=which.min(val.errors)
min
coef(regfit.best,min)
#attributes(CBCr)
regfit.fwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(rating~.,data=CBCr,nvmax=15,method="backward")
summary(regfit.bwd)
coef(regfit.full,6)
coef(regfit.fwd,6)
coef(regfit.bwd,6)
k=10
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
set.seed(1)
folds=sample(1:k,nrow(CBCr),replace=TRUE)
cv.errors=matrix(NA,k,15, dimnames=list(NULL, paste(1:15)))
for(j in 1:k){
best.fit=regsubsets(rating~.,data=CBCr[folds!=j,],nvmax=15)
for(i in 1:15){
pred=predict(best.fit,CBCr[folds==j,],id=i)
cv.errors[j,i]=mean( (CBC$rating[folds==j]-pred)^2)
}
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
min=which.min(mean.cv.errors)
min
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
reg.best=regsubsets(rating~.,data=CBCr, nvmax=15)
coef(reg.best,min)
library(glmnet)
CBC = read.csv(file = "Dati per caso Colonial Broadcasting.csv", header=T)
x=model.matrix(rating~.,CBCr)[,-1] #tolgo l'intercetta perchè Beta_0 non va penalizzato
y=CBC$rating
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
ridge.coef = predict(out,type="coefficients",s=bestlam)[1:16,]
ridge.coef
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:16,]
lasso.coef
plot(lasso.mod)
plot(ridge.mod)
ridge.coef
ridge.coef
CBC
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test))
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean(abs(ridge.pred-y.test))
mean(abs(ridge.pred-y.test))
# ----- PILGRIM BANK A
dtab = read.csv("06 - PilgrimABC.csv")
setwd("~/PoliTo/4 - Business Analytics/Script Business Analytics")
# ----- PILGRIM BANK A
dtab = read.csv("06 - PilgrimABC.csv")
#rinomico colonne per evitare problemi con i nomi
Profit=dtab$X9Profit
Online=dtab$X9Online
Age=dtab$X9Age
Income=dtab$X9Inc
Tenure=dtab$X9Tenure
District=dtab$X9District
summary(Profit)
#il primo quartile è negativo, vedo bene che è asimmetrico visto che media e mediana sono molto diversi
hist(Profit)
#c'è un discreto numero di clienti che mi fanno perdere, è molto scodato
#-------
N=length(Profit)
Nprofitable = sum(Profit>0)
cat('profitable = ', Nprofitable, ' out of ', N, '\n')
# Pareto curve
cumprofit=cumsum(sort(Profit,decreasing=TRUE))*100/sum(Profit)
plot(100*(1:N)/N,cumprofit,type='l')
grid()
setwd("~/PoliTo/4 - Business Analytics/Script Business Analytics/Tesina_Business_Analytics_CBC/PoliTo_Business_Analytics/Pilgrim Bank")
grid()
# mean profits
cat('average profit ', mean(Profit), '\n')
ProfitOnline = Profit[Online==1]
cat('average profit ON', mean(ProfitOnline), '\n') #verifichiamo quanto detto nel Business Case
ProfitOffline = Profit[Online==0]
cat('average profit OFF', mean(ProfitOffline), '\n')
#intervallo di confidenza per vedere se i dati sono sufficienti o no,
# se intervallo grande avrei bisogno di più clienti
cat('Conf int profit ', t.test(Profit)$conf.int, '\n')
cat('p-value difference ',t.test(ProfitOnline,ProfitOffline)$p.value, '\n')
mod = lm(Profit ~ Online)
summary(mod)
